{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPatel Web Scraping - Mars Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import requests\n",
    "import pymongo\n",
    "\n",
    "from splinter import Browser\n",
    "from splinter.exceptions import ElementDoesNotExist\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Define database and collection\n",
    "executable_path = {'executable_path': r\"C:\\Users\\Purvi Patel\\Documents\\Chromedriver\\chromedriver.exe\"}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "###########################################\n",
    "# MARS NEWS\n",
    "###########################################\n",
    "# URL of page to be scraped\n",
    "url1 = 'https://mars.nasa.gov/news/'\n",
    "\n",
    "# Retrieve the parent divs for all articles\n",
    "browser.visit(url1)\n",
    "html = browser.html\n",
    "soup = bs(html, 'html')\n",
    "\n",
    "#Retrieve the title of the first article\n",
    "first_article = soup.select_one(\"ul.item_list li.slide\")\n",
    "print(first_article)\n",
    "first_title = first_article.find('div', class_='content_title').get_text()\n",
    "print('####################################')\n",
    "print(first_title)\n",
    "\n",
    "# # ALTERNATIVE CODE FOR first_title\n",
    "# all_titles = soup.find_all('div', class_='content_title')\n",
    "# first_title = all_titles[1].text\n",
    "# print(first_title)\n",
    "\n",
    "#Save the article text\n",
    "first_paragraph = soup.find('div', class_='article_teaser_body').get_text()\n",
    "first_paragraph\n",
    "\n",
    "# # ALTERNATIVE CODE FOR first_title\n",
    "# first_paragraph = soup.find('div', class_ = 'article_teaser_body').text\n",
    "# print(first_paragraph)\n",
    "\n",
    "###########################################\n",
    "# FEATURED IMAGE\n",
    "###########################################\n",
    "url2 = 'https://mars.nasa.gov'\n",
    "browser.visit(url2)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "mars_featured_image = soup.find('img', class_='img-holder-img')\n",
    "image_name_section = soup.find('div', class_=\"window\")\n",
    "featured_image_name = image_name_section.find('h1', class_=\"media_feature_title\").text\n",
    "mars_img_url = mars_featured_image.get('src')\n",
    "\n",
    "featured_image_url = url2 + mars_img_url\n",
    "print(featured_image_url)\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "image_name_section\n",
    "find_img_page = image_name_section.find('a', class_='image_day')\n",
    "img_page = find_img_page.get('href')\n",
    "img_page\n",
    "\n",
    "url3 = img_page\n",
    "browser.visit(url3)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "text_sections = soup.find('div', id='body_content')\n",
    "text_content = text_sections.find_all('p')\n",
    "text_content\n",
    "\n",
    "img_description = []\n",
    "for p in text_content:\n",
    "    text = p.text\n",
    "    text = text.replace('\\n', '')\n",
    "    # print(text)\n",
    "    img_description.append(text)\n",
    "\n",
    "img_description\n",
    "\n",
    "###########################################\n",
    "# MARS FACTS\n",
    "###########################################\n",
    "url4 = 'https://space-facts.com/mars/'\n",
    "browser.visit(url4)\n",
    "\n",
    "mars_table = pd.read_html(url4)\n",
    "mars_table\n",
    "\n",
    "table_df = mars_table[0]\n",
    "table_df.columns = ['Measurement', 'Value']\n",
    "table_df.set_index('Measurement', inplace=True)\n",
    "table_df.rename(columns={\"Value\":\"\"})\n",
    "\n",
    "html_table = table_df.to_html(header = False)\n",
    "final_facts_table = html_table.replace('\\n', '')\n",
    "final_facts_table\n",
    "\n",
    "# IN CASE YOU NEED TO EXPORT THE CODE INTO A SEPARATE HTML FILE:\n",
    "# html_table = table_df.to_html(\"./facts_table.html\")\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "###########################################\n",
    "# MARS BACKGROUND INFO\n",
    "###########################################\n",
    "url5 = 'https://theplanets.org/mars/'\n",
    "browser.visit(url5)\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "intro_paragraph = soup.find('div', class_='entry-header-inner')\n",
    "texts = intro_paragraph.find_all('p')\n",
    "texts\n",
    "\n",
    "background = []\n",
    "for p in texts:\n",
    "    text = p.text\n",
    "    background.append(text)\n",
    "\n",
    "mars_background = background[1]\n",
    "mars_background\n",
    "\n",
    "###########################################\n",
    "# MARS HEMISPHERES\n",
    "###########################################\n",
    "\n",
    "#THIS WAS THE ORIGNAL CODE FOR THE PRIOR SCRAPE, BUT THE PAGE WAS AT ONE POINT DELETED BY NASA AND THE CODE WAS REWRITTEN FOR A NEW PAGE\n",
    "base = 'https://astrogeology.usgs.gov'\n",
    "url6 = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "browser.visit(url6)\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "mars_hemispheres_dict = []\n",
    "mars_hemipage_list = []\n",
    "mars_hemipage_links = []\n",
    "\n",
    "mars_hemisphere_data = soup.find_all('div', class_='description')\n",
    "hemi_name_tags = soup.find_all('h3')\n",
    "\n",
    "mars_hemispheres_dict.clear()\n",
    "mars_hemipage_list.clear()\n",
    "  \n",
    "for i in hemi_name_tags:\n",
    "    hemi_page_name = i.text\n",
    "    mars_hemipage_list.append(hemi_page_name)\n",
    "\n",
    "for i in mars_hemisphere_data:\n",
    "    link_url = i.a.get('href')\n",
    "    hemi_page_link = base + link_url\n",
    "    mars_hemipage_links.append(hemi_page_link)\n",
    "\n",
    "for i in mars_hemipage_list:\n",
    "    hemi_page = browser.find_link_by_partial_text(''+(i)+'')\n",
    "    hemi_page.click()\n",
    "    \n",
    "    html = browser.html\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    page_title = soup.find('h2').text\n",
    "    title_split = page_title.split((\" \"))\n",
    "    title_split.pop()\n",
    "    title = ' '.join(title_split)\n",
    "    title\n",
    "\n",
    "    img = soup.find('li')\n",
    "    img_url = img.a.get('href')\n",
    "    print(i)\n",
    "    print(img_url)\n",
    "\n",
    "    Hemisphere = {'title':title, 'img_url':img_url, 'hemi_page':i}\n",
    "    mars_hemispheres_dict.append(Hemisphere)\n",
    "    browser.back()\n",
    "\n",
    "mars_hemipage_links\n",
    "mars_hemispheres_dict\n",
    "\n",
    "# #THIS IS THE BACKUP CODE FROM ANOTHER SOURCE FOR MARS HEMISPHERE IMAGES, WHEN THE NASA PAEG IS NOT WORKING \n",
    "# url6 = 'https://www.planetary.org/blogs/guest-blogs/bill-dunford/20140203-the-faces-of-mars.html'\n",
    "\n",
    "# browser.visit(url6)\n",
    "# html = browser.html\n",
    "# soup = bs(html, 'html.parser')\n",
    "\n",
    "# mars_hemi_data = soup.find_all('img', class_='img840')\n",
    "\n",
    "# mars_hemispheres_dict = []\n",
    "\n",
    "# for i in mars_hemi_data:\n",
    "#     hemi_name_header = i.get('alt')\n",
    "#     header_split = hemi_name_header.split((\" \"))\n",
    "#     header_split.pop(0)\n",
    "#     title = ' '.join(header_split)\n",
    "#     print(title)\n",
    "\n",
    "#     img_url = i.get('src')\n",
    "#     print(img_url)\n",
    "    \n",
    "#     Hemisphere = {'title':title, 'img_url':img_url}\n",
    "#     mars_hemispheres_dict.append(Hemisphere)\n",
    "\n",
    "# # mars_hemispheres_dict.clear()\n",
    "# mars_hemispheres_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Weather -- scrape code will not work when joined in one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# MARS WEATHER\n",
    "###########################################\n",
    "url7 = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.visit(url7)\n",
    "\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "# # print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mars_weather_update = soup.select_one('article')\n",
    "# print(mars_weather_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather = mars_weather_update.find('div', lang='en').get_text()\n",
    "mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondae861a842cf624318a7290d59283a4372"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
